{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a9c662e-e1fa-4269-b66e-cf480d5fc7db",
   "metadata": {},
   "source": [
    "____________________________\n",
    "## This Python tool is designed to automatically analyze and compare Raman spectra from unknown and known polymer samples. It helps in identifying materials based on spectral similarity, combining peak-based matching and full-spectrum correlation\n",
    "\n",
    "____________________\n",
    "\n",
    "### - Automatic peak detection using scipy.signal.find_peaks\n",
    "\n",
    "### - Peak matching within a user-defined tolerance\n",
    "\n",
    "### - Full-spectrum (or selected range) comparison using Pearson correlation\n",
    "\n",
    "### - Weighted similarity scoring based on peak match and spectrum shape\n",
    "\n",
    "### - Visualization of best matches with overlaid spectra and metric breakdown\n",
    "\n",
    "### - Excel report generation with conditional formatting of matches\n",
    "\n",
    "### - Fast processing of multiple files using folder inputs\n",
    "\n",
    "__________________________\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8e6357-0635-43c6-af33-6a4d7d86cb27",
   "metadata": {},
   "source": [
    "## 1.0- Baseline Removal and Smoothing only (no peak picking)\n",
    "##### Do this step for both known and unknown signals if they are not corrected "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e654fa1-783e-4254-90cc-33e6b224f39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import sparse\n",
    "from scipy.sparse.linalg import spsolve\n",
    "from scipy.signal import savgol_filter\n",
    "import os\n",
    "from BaselineRemoval import BaselineRemoval\n",
    "import glob\n",
    "\n",
    "# Function to estimate baseline using ALS\n",
    "def baseline_als(y, lam, p, niter=10):\n",
    "    L = len(y)\n",
    "    D = sparse.csc_matrix(np.diff(np.eye(L), 2))\n",
    "    w = np.ones(L)\n",
    "    for i in range(niter):\n",
    "        W = sparse.spdiags(w, 0, L, L)\n",
    "        Z = W + lam * D.dot(D.transpose())\n",
    "        z = spsolve(Z, w*y)\n",
    "        w = p * (y > z) + (1-p) * (y < z)\n",
    "    return z\n",
    "\n",
    "# Function to load data from different file formats\n",
    "def load_data(file_path):\n",
    "    file_ext = os.path.splitext(file_path)[1].lower()\n",
    "    \n",
    "    if file_ext == '.csv':\n",
    "        data = pd.read_csv(file_path)\n",
    "    elif file_ext == '.xlsx':\n",
    "        data = pd.read_excel(file_path)\n",
    "    elif file_ext == '.txt':\n",
    "        data = pd.read_csv(file_path, sep='\\t')  # Assuming tab-separated values\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file format: {file_ext}\")\n",
    "    \n",
    "    # Check if the data has the expected columns\n",
    "    if 'Wave' not in data.columns or 'Intensity' not in data.columns:\n",
    "        # Try to handle files with different column names\n",
    "        if len(data.columns) >= 2:\n",
    "            # Rename the first two columns to 'Wave' and 'Intensity'\n",
    "            data = data.iloc[:, :2]\n",
    "            data.columns = ['Wave', 'Intensity']\n",
    "        else:\n",
    "            raise ValueError(f\"File does not have the required columns: {file_path}\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Function to process a single file and return the processed data\n",
    "def process_file(file_path, method, poly_degree, iters, conv_thresh, lambda_zhang, porder, repitition, \n",
    "                 lam_als, p_als, window_length, poly_order, output_dir, visualize=False):\n",
    "    \n",
    "    try:\n",
    "        # Load the data\n",
    "        data = load_data(file_path)\n",
    "        \n",
    "        # Apply baseline removal\n",
    "        if method == 'ALS':\n",
    "            baseline = baseline_als(data['Intensity'].values, lam_als, p_als)\n",
    "            corrected_intensity = data['Intensity'].values - baseline\n",
    "        else:\n",
    "            baseObj = BaselineRemoval(data['Intensity'].values)\n",
    "            if method == 'ModPoly':\n",
    "                corrected_intensity = baseObj.ModPoly(poly_degree, iters, conv_thresh)\n",
    "                baseline = data['Intensity'].values - corrected_intensity\n",
    "            elif method == 'IModPoly':\n",
    "                corrected_intensity = baseObj.IModPoly(poly_degree, iters, conv_thresh)\n",
    "                baseline = data['Intensity'].values - corrected_intensity\n",
    "            elif method == 'ZhangFit':\n",
    "                corrected_intensity = baseObj.ZhangFit(lambda_=lambda_zhang, porder=porder, repitition=repitition)\n",
    "                baseline = data['Intensity'].values - corrected_intensity\n",
    "        \n",
    "        # Apply smoothing\n",
    "        smoothed_output = savgol_filter(corrected_intensity, window_length, poly_order)\n",
    "        \n",
    "        # Create a DataFrame with processed data\n",
    "        processed_data = pd.DataFrame({\n",
    "            'Wave': data['Wave'],\n",
    "            'Original_Intensity': data['Intensity'],\n",
    "            'Baseline_Corrected': corrected_intensity,\n",
    "            'Smoothed': smoothed_output\n",
    "        })\n",
    "        \n",
    "        # Create output file path\n",
    "        file_name = os.path.basename(file_path)\n",
    "        file_base, file_ext = os.path.splitext(file_name)\n",
    "        output_file = os.path.join(output_dir, f\"{file_base}_bs+sm.xlsx\")\n",
    "        \n",
    "        # Save to Excel\n",
    "        processed_data.to_excel(output_file, index=False)\n",
    "        print(f\"Processed data saved to: {output_file}\")\n",
    "        \n",
    "        # Generate visualization if requested\n",
    "        if visualize:\n",
    "            plt.figure(figsize=(12, 12))\n",
    "            \n",
    "            # Plot the first set of data - Original, baseline, and corrected\n",
    "            plt.subplot(2, 1, 1)\n",
    "            plt.plot(data['Wave'], data['Intensity'], label='Original Data', color='grey')\n",
    "            plt.plot(data['Wave'], baseline, 'g--', label='Baseline')\n",
    "            plt.plot(data['Wave'], corrected_intensity, color='blue', label='Original Data - Baseline')\n",
    "            \n",
    "            plt.legend(fontsize=12)\n",
    "            plt.ylabel('Intensity', fontsize=12)\n",
    "            plt.title(f'Spectrum Analysis using {method} Method', fontsize=14)\n",
    "            \n",
    "            # Plot the second set of data - Smoothed data\n",
    "            plt.subplot(2, 1, 2)\n",
    "            plt.plot(data['Wave'], smoothed_output, 'b--', label='Smoothed Data')\n",
    "            \n",
    "            plt.legend(fontsize=12)\n",
    "            plt.xlabel('Raman Shift', fontsize=12)\n",
    "            plt.ylabel('Intensity', fontsize=12)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            \n",
    "            # Save the plot\n",
    "            plot_file = os.path.join(output_dir, f\"{file_base}_plot.png\")\n",
    "            plt.savefig(plot_file)\n",
    "            plt.close()\n",
    "            print(f\"Plot saved to: {plot_file}\")\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# Function to process all files in a folder\n",
    "def process_folder(input_dir, output_dir, method, poly_degree, iters, conv_thresh, lambda_zhang, porder, repitition,\n",
    "                   lam_als, p_als, window_length, poly_order):\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    # Get all files with supported extensions\n",
    "    file_patterns = [\n",
    "        os.path.join(input_dir, '*.csv'),\n",
    "        os.path.join(input_dir, '*.xlsx'),\n",
    "        os.path.join(input_dir, '*.txt')\n",
    "    ]\n",
    "    \n",
    "    all_files = []\n",
    "    for pattern in file_patterns:\n",
    "        all_files.extend(glob.glob(pattern))\n",
    "    \n",
    "    if not all_files:\n",
    "        print(f\"No supported files found in {input_dir}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(all_files)} files to process\")\n",
    "    \n",
    "    # Print parameter information\n",
    "    half_line = '-' * 40\n",
    "    stars = '*' * 40\n",
    "    print(stars)\n",
    "    print(\"Processing with these parameters:\")\n",
    "    print(stars)\n",
    "    print(f\"method: {method}\")\n",
    "    print(half_line)\n",
    "    print(\"Parameters for ModPoly and IModPoly:\")\n",
    "    print(f\"poly_degree: {poly_degree}\")\n",
    "    print(f\"iters: {iters}\")\n",
    "    print(f\"conv_thresh: {conv_thresh}\")\n",
    "    print(half_line)\n",
    "    print(\"Parameters for Zhang Method:\")\n",
    "    print(f\"lambda_zhang: {lambda_zhang}\")\n",
    "    print(f\"porder: {porder}\")\n",
    "    print(f\"repitition: {repitition}\")\n",
    "    print(half_line)\n",
    "    print(\"Parameters for ALS Method:\")\n",
    "    print(f\"lam_als: {lam_als}\")\n",
    "    print(f\"p_als: {p_als}\")\n",
    "    print(half_line)\n",
    "    print(\"Parameters for Smoothing:\")\n",
    "    print(f\"window_length: {window_length}\")\n",
    "    print(f\"poly_order: {poly_order}\")\n",
    "    print(stars)\n",
    "    \n",
    "    # Process all files\n",
    "    success_count = 0\n",
    "    for i, file_path in enumerate(all_files):\n",
    "        print(f\"Processing file {i+1}/{len(all_files)}: {os.path.basename(file_path)}\")\n",
    "        # Visualize the first file only\n",
    "        visualize = (i == 0)\n",
    "        if process_file(file_path, method, poly_degree, iters, conv_thresh, lambda_zhang, porder, repitition,\n",
    "                     lam_als, p_als, window_length, poly_order, output_dir, visualize):\n",
    "            success_count += 1\n",
    "    \n",
    "    print(f\"\\nProcessing complete: {success_count}/{len(all_files)} files processed successfully\")\n",
    "\n",
    "# Main function to run the batch processing\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Input and output directories\n",
    "    input_dir = r\"C:\\Raw\"  # Replace with the input directory\n",
    "    output_dir = r\"C:\\BL_Removed\"  # Replace with the output directory\n",
    "    \n",
    "    # Baseline removal method\n",
    "    method = 'ZhangFit'  # Can be 'ModPoly', 'IModPoly', 'ZhangFit', or 'ALS'\n",
    "    \n",
    "    # Parameters for ModPoly and IModPoly\n",
    "    poly_degree = 3\n",
    "    iters = 100\n",
    "    conv_thresh = 0.001\n",
    "    \n",
    "    # Parameters for ZhangFit\n",
    "    lambda_zhang = 100\n",
    "    porder = 3\n",
    "    repitition = 100\n",
    "    \n",
    "    # Parameters for ALS\n",
    "    lam_als = 10000\n",
    "    p_als = 0.001\n",
    "    \n",
    "    # Parameters for Savitzky-Golay filter\n",
    "    window_length = 21\n",
    "    poly_order = 3\n",
    "    \n",
    "    # Run the processing\n",
    "    process_folder(input_dir, output_dir, method, poly_degree, iters, conv_thresh, lambda_zhang, porder, repitition,\n",
    "                  lam_als, p_als, window_length, poly_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca2a0cb-3c29-4a31-a2d1-bf498e427361",
   "metadata": {},
   "source": [
    "## 2 - Compare spectra based on top N peaks and Pearson correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238049a6-d1f1-463e-9a30-0692ca513e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import find_peaks\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.interpolate import interp1d\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def find_top_peaks(df, wave_min, wave_max, num_peaks=10, exclude_ranges=None, name=None):\n",
    "    subset = df[(df[\"Wave\"] >= wave_min) & (df[\"Wave\"] <= wave_max)].copy()\n",
    "    if len(subset) < 2:\n",
    "        return None, None, None, None\n",
    "\n",
    "    if subset[\"Smoothed\"].max() != subset[\"Smoothed\"].min():\n",
    "        subset[\"Normalized\"] = (subset[\"Smoothed\"] - subset[\"Smoothed\"].min()) / (subset[\"Smoothed\"].max() - subset[\"Smoothed\"].min())\n",
    "    else:\n",
    "        subset[\"Normalized\"] = subset[\"Smoothed\"]\n",
    "\n",
    "    #prominence threshold (5% of the normalized range) -- if needed you can choose 0.05 or any percentage instead of zero\n",
    "    prominence_threshold = 0.00 * (subset[\"Normalized\"].max() - subset[\"Normalized\"].min())\n",
    "    \n",
    "    peaks, _ = find_peaks(subset[\"Normalized\"], distance=5, prominence=prominence_threshold)\n",
    "    if len(peaks) == 0:\n",
    "        return None, None, None, None\n",
    "\n",
    "    peak_positions = subset[\"Wave\"].iloc[peaks].values\n",
    "    peak_intensities = subset[\"Normalized\"].iloc[peaks].values\n",
    "\n",
    "    # Exclusion of ranges\n",
    "    if exclude_ranges:\n",
    "        for r_min, r_max in exclude_ranges:\n",
    "            exclude_mask = (peak_positions < r_min) | (peak_positions > r_max)\n",
    "            peak_positions = peak_positions[exclude_mask]\n",
    "            peak_intensities = peak_intensities[exclude_mask]\n",
    "\n",
    "    if len(peak_positions) == 0:\n",
    "        return subset, [], np.array([]), np.array([]), 0\n",
    "\n",
    "    top_indices = np.argsort(peak_intensities)[::-1][:num_peaks]\n",
    "    top_positions = peak_positions[top_indices]\n",
    "    top_intensities = peak_intensities[top_indices]\n",
    "\n",
    "    actual_peaks_used = len(top_positions)\n",
    "\n",
    "    return subset, None, top_positions, top_intensities, actual_peaks_used\n",
    "\n",
    "\n",
    "def save_peaks_vertical_grouped(peaks_dict, output_peaks_file, group_size=30, file_prefix=\"\"):\n",
    "    from openpyxl import Workbook\n",
    "\n",
    "    peak_names = sorted(peaks_dict.keys())\n",
    "    wb = Workbook()\n",
    "    wb.remove(wb.active)\n",
    "\n",
    "    sheet_count = 0\n",
    "    for group_start in range(0, len(peak_names), group_size):\n",
    "        sheet_count += 1\n",
    "        ws = wb.create_sheet(title=f\"Group_{sheet_count}\")\n",
    "        row_idx = 1\n",
    "\n",
    "        for name in peak_names[group_start:group_start + group_size]:\n",
    "            result = peaks_dict[name]\n",
    "            if result is None:\n",
    "                continue\n",
    "            _, _, peak_positions, peak_intensities, _ = result  \n",
    "            peak_df = pd.DataFrame({\n",
    "                \"Peak Position (cm⁻¹)\": peak_positions,\n",
    "                \"Peak Intensity (Normalized)\": peak_intensities\n",
    "            }).sort_values(by=\"Peak Position (cm⁻¹)\").reset_index(drop=True)\n",
    "\n",
    "            ws.cell(row=row_idx, column=1, value=f\"{file_prefix}{name}\")\n",
    "            row_idx += 1\n",
    "            ws.cell(row=row_idx, column=1, value=\"Peak Position (cm⁻¹)\")\n",
    "            ws.cell(row=row_idx, column=2, value=\"Peak Intensity (Normalized)\")\n",
    "            row_idx += 1\n",
    "\n",
    "            for _, row in peak_df.iterrows():\n",
    "                ws.cell(row=row_idx, column=1, value=row[\"Peak Position (cm⁻¹)\"])\n",
    "                ws.cell(row=row_idx, column=2, value=row[\"Peak Intensity (Normalized)\"])\n",
    "                row_idx += 1\n",
    "\n",
    "            row_idx += 1\n",
    "\n",
    "    wb.save(output_peaks_file)\n",
    "    print(f\"\\nGrouped and sorted peak data saved to {output_peaks_file}\")\n",
    "\n",
    "\n",
    "def save_unknown_peaks_vertical_grouped(unknown_peaks_dict, output_peaks_file, group_size=30):\n",
    "    save_peaks_vertical_grouped(unknown_peaks_dict, output_peaks_file, group_size, \"Polymer: \")\n",
    "\n",
    "\n",
    "def save_known_peaks_vertical_grouped(known_peaks_dict, output_peaks_file, group_size=30):\n",
    "    save_peaks_vertical_grouped(known_peaks_dict, output_peaks_file, group_size, \"Known Polymer: \")\n",
    "\n",
    "\n",
    "def compare_raman_spectra(unknown_df, known_result, peak_tolerance=10, num_top_peaks=10,\n",
    "                          range_min=700, range_max=1700, peak_weight=0.70, pearson_weight=0.30,\n",
    "                          exclude_ranges=None):\n",
    "    total_weight = peak_weight + pearson_weight\n",
    "    if total_weight != 1.0:\n",
    "        peak_weight /= total_weight\n",
    "        pearson_weight /= total_weight\n",
    "\n",
    "    unknown_result = find_top_peaks(unknown_df, range_min, range_max, num_top_peaks, exclude_ranges=exclude_ranges)\n",
    "    if unknown_result[0] is None or known_result[0] is None:\n",
    "        return None\n",
    "\n",
    "    unknown_subset, _, unknown_peaks, _, _ = unknown_result  \n",
    "    known_subset, _, known_peaks, _, _ = known_result  \n",
    "\n",
    "    if len(unknown_peaks) > 0 and len(known_peaks) > 0:\n",
    "        distances = np.abs(unknown_peaks[:, np.newaxis] - known_peaks)\n",
    "        matched_peaks = np.sum(np.any(distances <= peak_tolerance, axis=1))\n",
    "        \n",
    "        denominator = max(len(unknown_peaks), len(known_peaks))        \n",
    "        # Ensure denominator is not less than numerator\n",
    "        denominator = max(denominator, matched_peaks)\n",
    "        \n",
    "        peak_score = (matched_peaks / denominator) * 100 if denominator > 0 else 0\n",
    "    else:\n",
    "        peak_score = 0\n",
    "\n",
    "    xmin = max(unknown_subset[\"Wave\"].min(), known_subset[\"Wave\"].min())\n",
    "    xmax = min(unknown_subset[\"Wave\"].max(), known_subset[\"Wave\"].max())\n",
    "    if xmax <= xmin:\n",
    "        return 0\n",
    "\n",
    "    try:\n",
    "        f1 = interp1d(unknown_subset[\"Wave\"], unknown_subset[\"Normalized\"], kind='linear', bounds_error=True)\n",
    "        f2 = interp1d(known_subset[\"Wave\"], known_subset[\"Normalized\"], kind='linear', bounds_error=True)\n",
    "        x_common = np.linspace(xmin, np.nextafter(xmax, xmin), 1000)\n",
    "        y1 = f1(x_common)\n",
    "        y2 = f2(x_common)\n",
    "        corr, _ = pearsonr(y1, y2)\n",
    "        pearson_score = (corr + 1) * 50\n",
    "    except:\n",
    "        pearson_score = 0\n",
    "\n",
    "    final_score = peak_weight * peak_score + pearson_weight * pearson_score\n",
    "    return final_score\n",
    "\n",
    "\n",
    "def plot_best_match(unknown_result, known_result, unknown_name, known_name, similarity_score,\n",
    "                    peak_tolerance, num_top_peaks, range_min, range_max,\n",
    "                    peak_weight, pearson_weight, threshold, output_folder):\n",
    "    unknown_subset, _, unknown_pos, unknown_int, _ = unknown_result  \n",
    "    known_subset, _, known_pos, known_int, _ = known_result  \n",
    "\n",
    "    if len(unknown_pos) > 0 and len(known_pos) > 0:\n",
    "        distances = np.abs(unknown_pos[:, np.newaxis] - known_pos)\n",
    "        matched_peaks = np.sum(np.any(distances <= peak_tolerance, axis=1))\n",
    "        \n",
    "        denominator = len(unknown_pos)    \n",
    "        # Ensure denominator is not less than numerator\n",
    "        denominator = max(denominator, matched_peaks)\n",
    "        \n",
    "        peak_score = (matched_peaks / denominator) * 100 if denominator > 0 else 0\n",
    "    else:\n",
    "        peak_score = 0\n",
    "\n",
    "    xmin = max(unknown_subset[\"Wave\"].min(), known_subset[\"Wave\"].min())\n",
    "    xmax = min(unknown_subset[\"Wave\"].max(), known_subset[\"Wave\"].max())\n",
    "\n",
    "    try:\n",
    "        f1 = interp1d(unknown_subset[\"Wave\"], unknown_subset[\"Normalized\"], kind='linear', bounds_error=True)\n",
    "        f2 = interp1d(known_subset[\"Wave\"], known_subset[\"Normalized\"], kind='linear', bounds_error=True)\n",
    "        x_common = np.linspace(xmin, np.nextafter(xmax, xmin), 1000)\n",
    "        y1 = f1(x_common)\n",
    "        y2 = f2(x_common)\n",
    "        corr, _ = pearsonr(y1, y2)\n",
    "        pearson_score = (corr + 1) * 50\n",
    "    except:\n",
    "        pearson_score = 0\n",
    "        corr = 0\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(unknown_subset[\"Wave\"], unknown_subset[\"Normalized\"], label=f\"Unknown: {unknown_name}\", alpha=0.7)\n",
    "    plt.plot(known_subset[\"Wave\"], known_subset[\"Normalized\"], label=f\"Known: {known_name}\", alpha=0.7)\n",
    "    plt.scatter(unknown_pos, unknown_int, color='blue', marker='x', label=\"Unknown Peaks\")\n",
    "    plt.scatter(known_pos, known_int, color='red', marker='o', label=\"Known Peaks\")\n",
    "    plt.figtext(0.5, 0.45, f\"Full Spectrum Correlation: r={corr:.2f}\", ha='center', fontsize=10, bbox=dict(facecolor='white', alpha=0.8))\n",
    "    plt.xlabel(\"Raman Shift (cm⁻¹)\")\n",
    "    plt.ylabel(\"Normalized Intensity\")\n",
    "    plt.title(f\"{unknown_name} vs {known_name} (Similarity: {similarity_score:.2f}%)\")\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.xlim(range_min, range_max)\n",
    "    plt.ylim(0, 1.05)\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    metrics = ['Peak Match', 'Full Spectrum Corr.', 'Total Similarity']\n",
    "    scores = [peak_score, pearson_score, similarity_score]\n",
    "    weights = [peak_weight * 100, pearson_weight * 100, 100]\n",
    "    plt.bar(metrics, scores, alpha=0.7)\n",
    "    plt.axhline(y=threshold, color='r', linestyle='--', alpha=0.7, label=f'{threshold}% Threshold')\n",
    "    for i, v in enumerate(scores):\n",
    "        plt.text(i, v + 2, f\"{v:.2f}%\\n(w={weights[i]:.0f}%)\", ha='center', va='bottom', fontsize=9)\n",
    "    plt.ylabel(\"Score (%)\")\n",
    "    plt.title(\"Similarity Metrics (Full Spectrum)\")\n",
    "    plt.ylim(0, 105)\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    if output_folder:\n",
    "        plt.savefig(os.path.join(output_folder, f\"{unknown_name}_vs_{known_name}.png\"))\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def analyze_folders(unknown_folder, known_folder, output_file,\n",
    "                    threshold=70, visualize=True, peak_tolerance=10,\n",
    "                    num_top_peaks=10, range_min=700, range_max=1700,\n",
    "                    peak_weight=0.70, pearson_weight=0.30,\n",
    "                    plot_folder=None, exclude_ranges=None):\n",
    "    results = []\n",
    "    unknown_files = [f for f in os.listdir(unknown_folder) if f.endswith('.xlsx')]\n",
    "    known_files = [f for f in os.listdir(known_folder) if f.endswith('.xlsx')]\n",
    "\n",
    "    if visualize and plot_folder and not os.path.exists(plot_folder):\n",
    "        os.makedirs(plot_folder)\n",
    "\n",
    "    all_peak_counts = []\n",
    "\n",
    "    known_precomputed = {}\n",
    "    for f in tqdm(known_files, desc=\"Precomputing knowns\"):\n",
    "        df = pd.read_excel(os.path.join(known_folder, f))\n",
    "        name = f.replace('_bs+sm.xlsx', '')\n",
    "        result = find_top_peaks(df, range_min, range_max, num_top_peaks,\n",
    "                               exclude_ranges=exclude_ranges, name=name)\n",
    "        known_precomputed[name] = result\n",
    "        if result[4] is not None:  \n",
    "            all_peak_counts.append(result[4])\n",
    "\n",
    "    unknown_peaks_dict = {}\n",
    "    for f in unknown_files:\n",
    "        df = pd.read_excel(os.path.join(unknown_folder, f))\n",
    "        name = f.replace('_bs+sm.xlsx', '')\n",
    "        result = find_top_peaks(df, range_min, range_max, num_top_peaks,\n",
    "                               exclude_ranges=exclude_ranges, name=name)\n",
    "        unknown_peaks_dict[name] = result\n",
    "        if result[4] is not None:  \n",
    "            all_peak_counts.append(result[4])\n",
    "\n",
    "    for u_file in tqdm(unknown_files, desc=\"Processing unknowns\"):\n",
    "        u_name = u_file.replace('_bs+sm.xlsx', '')\n",
    "        u_df = pd.read_excel(os.path.join(unknown_folder, u_file))\n",
    "        for k_name, k_result in known_precomputed.items():\n",
    "            if u_file == f\"{k_name}_bs+sm.xlsx\":\n",
    "                continue\n",
    "            score = compare_raman_spectra(u_df, k_result, peak_tolerance, num_top_peaks,\n",
    "                                          range_min, range_max, peak_weight, pearson_weight,\n",
    "                                          exclude_ranges=exclude_ranges)\n",
    "            if score is not None:\n",
    "                results.append([k_name, u_name, score])\n",
    "\n",
    "    results_df = pd.DataFrame(results, columns=[\"Known Polymer\", \"Unknown Polymer\", \"Similarity (%)\"])\n",
    "    best_matches = {}\n",
    "    for u_name in set(results_df[\"Unknown Polymer\"]):\n",
    "        matches = results_df[results_df[\"Unknown Polymer\"] == u_name]\n",
    "        if not matches.empty:\n",
    "            best_row = matches.loc[matches[\"Similarity (%)\"].idxmax()]\n",
    "            k_name = best_row[\"Known Polymer\"]\n",
    "            score = best_row[\"Similarity (%)\"]\n",
    "            best_matches[u_name] = (k_name if score >= threshold else None, score)\n",
    "            if visualize and k_name:\n",
    "                plot_best_match(unknown_peaks_dict[u_name], known_precomputed[k_name], u_name, k_name, score,\n",
    "                                peak_tolerance, num_top_peaks, range_min, range_max,\n",
    "                                peak_weight, pearson_weight, threshold, plot_folder)\n",
    "\n",
    "    pivot = results_df.pivot(index=\"Known Polymer\", columns=\"Unknown Polymer\", values=\"Similarity (%)\")\n",
    "    best_row = pd.Series({u: best_matches[u][0] if best_matches[u][0] else \"\" for u in best_matches}, name=\"Best Match\")\n",
    "    final_df = pd.concat([pivot, best_row.to_frame().T])\n",
    "\n",
    "    with pd.ExcelWriter(output_file, engine='xlsxwriter') as writer:\n",
    "        final_df.to_excel(writer, sheet_name='Results')\n",
    "        wb = writer.book\n",
    "        ws = writer.sheets['Results']\n",
    "        percent_fmt = wb.add_format({'num_format': '0.00'})\n",
    "        highlight_fmt = wb.add_format({'num_format': '0.00', 'bold': True, 'bg_color': 'yellow'})\n",
    "        for r in range(1, final_df.shape[0]):\n",
    "            for c in range(1, final_df.shape[1] + 1):\n",
    "                val = final_df.iloc[r - 1, c - 1]\n",
    "                if pd.notna(val) and isinstance(val, (int, float)):\n",
    "                    fmt = highlight_fmt if val >= threshold else percent_fmt\n",
    "                    ws.write(r, c, val, fmt)\n",
    "\n",
    "    print(f\"\\nResults saved to {output_file}\")\n",
    "    \n",
    "    # Print peak count summary (actual peaks used in comparison)\n",
    "    if all_peak_counts:\n",
    "        from collections import Counter\n",
    "        peak_counts = Counter(all_peak_counts)\n",
    "        print(f\"\\n📊 Peak Detection Summary (peaks used in comparison):\")\n",
    "        print(f\"Requested peaks per sample: {num_top_peaks}\")\n",
    "        for peak_count, count in sorted(peak_counts.items()):\n",
    "            print(f\"  {count} samples used {peak_count} peaks\")\n",
    "    \n",
    "    return unknown_peaks_dict, known_precomputed\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    unknown_folder = r'C:\\Unkonw' #Unknown polymers folder but make sure use them one after the baseline removal and smoothing from step 1.0\n",
    "    known_folder = r'C:\\Known' #known polymers folder but make sure to use them after the baseline removal and smoothing from step 1.0\n",
    "    output_file = r'C:\\Output.xlsx' #Excel file to save the results\n",
    "    plot_folder = r'C:\\PLot' #Folder to save the Plots\n",
    "    peak_output_file_grouped = r'C:\\output_peaks_unknown_vertical_grouped.xlsx' #Excel file to Recored te unkown polymers peaks\n",
    "    known_peak_output_file_grouped =  r'C:\\output_peaks_known_vertical_grouped.xlsx' #Excel file to Recored te kown polymers peaks\n",
    "\n",
    "    range_min = 800 # Choose the polymer range you are intrested in\n",
    "    range_max = 1800 # Choose the polymer range you are intrested in\n",
    "    num_top_peaks = 12 # Choose the number of peaks for the analysis\n",
    "    exclude_ranges = [(x1,x2)] # Exclude this range [x1 to x2] from the analysis\n",
    "\n",
    "    unknown_peaks_dict, known_peaks_dict = analyze_folders(\n",
    "        unknown_folder,\n",
    "        known_folder,\n",
    "        output_file,\n",
    "        threshold=70, # Change the threshold if needed\n",
    "        visualize=True,\n",
    "        peak_tolerance=10,\n",
    "        num_top_peaks=num_top_peaks,\n",
    "        range_min=range_min,\n",
    "        range_max=range_max,\n",
    "        peak_weight=0.70,  # Change the weight if needed\n",
    "        pearson_weight=0.30, # Change the weight if needed\n",
    "        plot_folder=plot_folder,\n",
    "        exclude_ranges=exclude_ranges\n",
    "    )\n",
    "\n",
    "    # Save unknown peaks\n",
    "    save_unknown_peaks_vertical_grouped(\n",
    "        unknown_peaks_dict,\n",
    "        peak_output_file_grouped,\n",
    "        group_size=30\n",
    "    )\n",
    "\n",
    "    # Save known peaks\n",
    "    save_known_peaks_vertical_grouped(\n",
    "        known_peaks_dict,\n",
    "        known_peak_output_file_grouped,\n",
    "        group_size=30\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cc6a71-bff7-4c9d-9ea3-976a52e96313",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
